{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Submissions are scored on the root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://www.kaggle.com/code/sebastianvangerwen/1st-place-solution-tps-jun-denoising-ae issued by **@SEBASTIAN VAN GERWEN**<br>\n",
    "https://towardsdatascience.com/denoising-autoencoders-dae-how-to-use-neural-networks-to-clean-up-your-data-cd9c19bc6915 issued by **@Saul Dobilas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blue Print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 81)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 81 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   row_id  1000000 non-null  int64  \n",
      " 1   F_1_0   981603 non-null   float64\n",
      " 2   F_1_1   981784 non-null   float64\n",
      " 3   F_1_2   981992 non-null   float64\n",
      " 4   F_1_3   981750 non-null   float64\n",
      " 5   F_1_4   981678 non-null   float64\n",
      " 6   F_1_5   981911 non-null   float64\n",
      " 7   F_1_6   981867 non-null   float64\n",
      " 8   F_1_7   981872 non-null   float64\n",
      " 9   F_1_8   981838 non-null   float64\n",
      " 10  F_1_9   981751 non-null   float64\n",
      " 11  F_1_10  982039 non-null   float64\n",
      " 12  F_1_11  981830 non-null   float64\n",
      " 13  F_1_12  981797 non-null   float64\n",
      " 14  F_1_13  981602 non-null   float64\n",
      " 15  F_1_14  981961 non-null   float64\n",
      " 16  F_2_0   1000000 non-null  int64  \n",
      " 17  F_2_1   1000000 non-null  int64  \n",
      " 18  F_2_2   1000000 non-null  int64  \n",
      " 19  F_2_3   1000000 non-null  int64  \n",
      " 20  F_2_4   1000000 non-null  int64  \n",
      " 21  F_2_5   1000000 non-null  int64  \n",
      " 22  F_2_6   1000000 non-null  int64  \n",
      " 23  F_2_7   1000000 non-null  int64  \n",
      " 24  F_2_8   1000000 non-null  int64  \n",
      " 25  F_2_9   1000000 non-null  int64  \n",
      " 26  F_2_10  1000000 non-null  int64  \n",
      " 27  F_2_11  1000000 non-null  int64  \n",
      " 28  F_2_12  1000000 non-null  int64  \n",
      " 29  F_2_13  1000000 non-null  int64  \n",
      " 30  F_2_14  1000000 non-null  int64  \n",
      " 31  F_2_15  1000000 non-null  int64  \n",
      " 32  F_2_16  1000000 non-null  int64  \n",
      " 33  F_2_17  1000000 non-null  int64  \n",
      " 34  F_2_18  1000000 non-null  int64  \n",
      " 35  F_2_19  1000000 non-null  int64  \n",
      " 36  F_2_20  1000000 non-null  int64  \n",
      " 37  F_2_21  1000000 non-null  int64  \n",
      " 38  F_2_22  1000000 non-null  int64  \n",
      " 39  F_2_23  1000000 non-null  int64  \n",
      " 40  F_2_24  1000000 non-null  int64  \n",
      " 41  F_3_0   981971 non-null   float64\n",
      " 42  F_3_1   981655 non-null   float64\n",
      " 43  F_3_2   981944 non-null   float64\n",
      " 44  F_3_3   981946 non-null   float64\n",
      " 45  F_3_4   981627 non-null   float64\n",
      " 46  F_3_5   981702 non-null   float64\n",
      " 47  F_3_6   981808 non-null   float64\n",
      " 48  F_3_7   981987 non-null   float64\n",
      " 49  F_3_8   981902 non-null   float64\n",
      " 50  F_3_9   981894 non-null   float64\n",
      " 51  F_3_10  981800 non-null   float64\n",
      " 52  F_3_11  981612 non-null   float64\n",
      " 53  F_3_12  981703 non-null   float64\n",
      " 54  F_3_13  981940 non-null   float64\n",
      " 55  F_3_14  981861 non-null   float64\n",
      " 56  F_3_15  981762 non-null   float64\n",
      " 57  F_3_16  981878 non-null   float64\n",
      " 58  F_3_17  981722 non-null   float64\n",
      " 59  F_3_18  981911 non-null   float64\n",
      " 60  F_3_19  981800 non-null   float64\n",
      " 61  F_3_20  981752 non-null   float64\n",
      " 62  F_3_21  981604 non-null   float64\n",
      " 63  F_3_22  981823 non-null   float64\n",
      " 64  F_3_23  981794 non-null   float64\n",
      " 65  F_3_24  981855 non-null   float64\n",
      " 66  F_4_0   981872 non-null   float64\n",
      " 67  F_4_1   981836 non-null   float64\n",
      " 68  F_4_2   981505 non-null   float64\n",
      " 69  F_4_3   981971 non-null   float64\n",
      " 70  F_4_4   982043 non-null   float64\n",
      " 71  F_4_5   981937 non-null   float64\n",
      " 72  F_4_6   981675 non-null   float64\n",
      " 73  F_4_7   981986 non-null   float64\n",
      " 74  F_4_8   981824 non-null   float64\n",
      " 75  F_4_9   981735 non-null   float64\n",
      " 76  F_4_10  981775 non-null   float64\n",
      " 77  F_4_11  981881 non-null   float64\n",
      " 78  F_4_12  981694 non-null   float64\n",
      " 79  F_4_13  982005 non-null   float64\n",
      " 80  F_4_14  981733 non-null   float64\n",
      "dtypes: float64(55), int64(26)\n",
      "memory usage: 618.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Column `F_1_0` ~ `F_1_14`, `F_3_0` ~ `F_3_24`, `F_4_0` ~ `F_4_14` have missing values. The types of missing values are all floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F_1_0',\n",
       " 'F_1_1',\n",
       " 'F_1_2',\n",
       " 'F_1_3',\n",
       " 'F_1_4',\n",
       " 'F_1_5',\n",
       " 'F_1_6',\n",
       " 'F_1_7',\n",
       " 'F_1_8',\n",
       " 'F_1_9',\n",
       " 'F_1_10',\n",
       " 'F_1_11',\n",
       " 'F_1_12',\n",
       " 'F_1_13',\n",
       " 'F_1_14',\n",
       " 'F_2_0',\n",
       " 'F_2_1',\n",
       " 'F_2_2',\n",
       " 'F_2_3',\n",
       " 'F_2_4',\n",
       " 'F_2_5',\n",
       " 'F_2_6',\n",
       " 'F_2_7',\n",
       " 'F_2_8',\n",
       " 'F_2_9',\n",
       " 'F_2_10',\n",
       " 'F_2_11',\n",
       " 'F_2_12',\n",
       " 'F_2_13',\n",
       " 'F_2_14',\n",
       " 'F_2_15',\n",
       " 'F_2_16',\n",
       " 'F_2_17',\n",
       " 'F_2_18',\n",
       " 'F_2_19',\n",
       " 'F_2_20',\n",
       " 'F_2_21',\n",
       " 'F_2_22',\n",
       " 'F_2_23',\n",
       " 'F_2_24',\n",
       " 'F_3_0',\n",
       " 'F_3_1',\n",
       " 'F_3_2',\n",
       " 'F_3_3',\n",
       " 'F_3_4',\n",
       " 'F_3_5',\n",
       " 'F_3_6',\n",
       " 'F_3_7',\n",
       " 'F_3_8',\n",
       " 'F_3_9',\n",
       " 'F_3_10',\n",
       " 'F_3_11',\n",
       " 'F_3_12',\n",
       " 'F_3_13',\n",
       " 'F_3_14',\n",
       " 'F_3_15',\n",
       " 'F_3_16',\n",
       " 'F_3_17',\n",
       " 'F_3_18',\n",
       " 'F_3_19',\n",
       " 'F_3_20',\n",
       " 'F_3_21',\n",
       " 'F_3_22',\n",
       " 'F_3_23',\n",
       " 'F_3_24',\n",
       " 'F_4_0',\n",
       " 'F_4_1',\n",
       " 'F_4_2',\n",
       " 'F_4_3',\n",
       " 'F_4_4',\n",
       " 'F_4_5',\n",
       " 'F_4_6',\n",
       " 'F_4_7',\n",
       " 'F_4_8',\n",
       " 'F_4_9',\n",
       " 'F_4_10',\n",
       " 'F_4_11',\n",
       " 'F_4_12',\n",
       " 'F_4_13',\n",
       " 'F_4_14']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of features\n",
    "features = data.columns.drop('row_id').tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_col, f2_col, f3_col, f4_col = [], [], [], []\n",
    "for f in features:\n",
    "    if f[:3] == 'F_1':\n",
    "        f1_col.append(f)\n",
    "    elif f[:3] == 'F_2':\n",
    "        f2_col.append(f)\n",
    "    elif f[:3] == 'F_3':\n",
    "        f3_col.append(f)\n",
    "    elif f[:3] == 'F_4':\n",
    "        f4_col.append(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binomial x Random one 0 per row\n",
    "def random_mask(n, k):\n",
    "    mask = np.ones((n, k))\n",
    "\n",
    "    # Set one random per row at 0\n",
    "    mask[(np.arange(n), np.random.randint(0, k, n))] = 0\n",
    "    \n",
    "    # Add binomial probability as well\n",
    "    b_mask = np.random.binomial(1, 0.5, (n, k))    # 1 trial, p=0.5\n",
    "    return mask * b_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_n_rows(n, k, n_missing):\n",
    "    # n_missing number of indices of columns with small values\n",
    "    idx = np.random.rand(n, k).argsort(1)[:, :n_missing]\n",
    "\n",
    "    col_idx = idx.flatten()\n",
    "    row_idx = np.arange(n).repeat(n_missing)\n",
    "    \n",
    "    mask = np.ones((n, k))\n",
    "    mask[(row_idx, col_idx)] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with missing values\n",
    "missing_bool = data[features].isna().sum(axis=1) > 0\n",
    "\n",
    "# Define subsets of the data with row-wise missing values\n",
    "complete = data.loc[~missing_bool, features].values     # without missing values\n",
    "missing = data.loc[missing_bool, features].values       # with missing values\n",
    "\n",
    "# Split data that has no missing to use for validation set\n",
    "Xtr, Xva = train_test_split(complete)\n",
    "\n",
    "# Build training set by combining cXtr and missing data\n",
    "X_mixed = np.concatenate([Xtr, missing], axis=0)\n",
    "\n",
    "# Mask to show train values that have been imputed\n",
    "srce_nan_train = np.concatenate([\n",
    "    np.zeros(Xtr.shape),\n",
    "    data.loc[missing_bool, features].isna().astype(np.uint8).values])\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data[features].values)\n",
    "\n",
    "# Apply feature scaling\n",
    "X_mixed = np.nan_to_num(scaler.transform(X_mixed), 0.0)\n",
    "Xva = scaler.transform(Xva)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "# Dense layer with layer normalization and mish activation\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size)\n",
    "        self.act = nn.Mish()\n",
    "        self.layernorm = nn.LayerNorm(output_size, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.act(x)\n",
    "        return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Msked autoencoder model\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, n_columns, emb_dim=16,\n",
    "                 units=[512, 512, 512, 512, 512, 128]):\n",
    "        super().__init__()\n",
    "        self.n_columns = n_columns\n",
    "\n",
    "        # Embedding\n",
    "        self.inp_proj = nn.Linear(1, emb_dim)\n",
    "        self.mask_proj = nn.Linear(1, emb_dim)\n",
    "        self.emb_norm = nn.LayerNorm(n_columns * emb_dim, eps=1e-6)\n",
    "        \n",
    "        # MLP with skip connection\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        for i in range(len(units)):\n",
    "            if i==0:\n",
    "                input_size = n_columns * emb_dim\n",
    "            elif i==1:\n",
    "                input_size = n_columns * emb_dim + units[0]\n",
    "            else:\n",
    "                input_size = units[i-1] + units[i-2]\n",
    "            output_size = units[i]\n",
    "            self.mlp_layers.append(\n",
    "                MLP(input_size=input_size, output_size=output_size)\n",
    "            )\n",
    "                \n",
    "        self.final_dense = nn.Linear(units[-1] + units[-2], self.n_columns)\n",
    "        \n",
    "    def forward(self, inputs:torch.Tensor, mask:torch.Tensor):\n",
    "        # Embeddings\n",
    "        input_embedding = self.inp_proj(torch.unsqueeze(inputs, 2))\n",
    "        mask_embedding = self.mask_proj(torch.unsqueeze(1-mask, 2))\n",
    "        embedding = input_embedding + mask_embedding\n",
    "        embedding = torch.flatten(embedding, 1)\n",
    "        x = [self.emb_norm(embedding)]\n",
    "        \n",
    "        # MLP\n",
    "        for i in range(len(self.mlp_layers)):\n",
    "            if i==0:\n",
    "                z = self.mlp_layers[i](x[0])\n",
    "                x.append(z)\n",
    "            else:\n",
    "                z = torch.cat((x[-1], x[-2]), 1)\n",
    "                z = self.mlp_layers[i](z)\n",
    "                x.append(z)\n",
    "                \n",
    "        x = torch.cat((x[-1], x[-2]), 1)\n",
    "        x = self.final_dense(x)\n",
    "        \n",
    "        # Output modification - predict only masked values, otherwise use inputs\n",
    "        outputs = torch.mul(inputs, mask) + torch.mul(1-mask, x)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper validation method\n",
    "def validate(model, valid_mask, batch_size=4096):\n",
    "    assert valid_mask.shape == Xva.shape\n",
    "    \n",
    "    n_batches_valid = Xva.shape[0] // batch_size + 1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ps = []\n",
    "        for batch in range(n_batches_valid):\n",
    "            x = torch.tensor(Xva[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            mask = torch.tensor(valid_mask[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            x_masked = x * mask\n",
    "\n",
    "            p = model(x_masked, mask).cpu().numpy()\n",
    "            ps.append(p)\n",
    "\n",
    "        p = np.vstack(ps)\n",
    "        mask_bool = (1 - valid_mask).astype(bool)\n",
    "        rmse = np.sqrt(mean_squared_error(\n",
    "            scaler.inverse_transform(p)[mask_bool],\n",
    "            scaler.inverse_transform(Xva)[mask_bool]\n",
    "        ))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to mask NaNs in the original data\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    # Mask should be 1 for masked value, 0 for unmasked value \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, inputs, target, mask):\n",
    "        loss = self.loss(inputs, target)\n",
    "        return torch.mean(loss * (1 - mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model parameters and learning rate schedule\n",
    "epochs = 300\n",
    "lr_start = 0.001\n",
    "lr_end = 0.00005\n",
    "batch_size = 4096\n",
    "\n",
    "# This cosine decay function is borrowed from AmbrosM in last month's TPS\n",
    "def cosine_decay(epoch):\n",
    "    if epochs > 1:\n",
    "        w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n",
    "    else:\n",
    "        w = 1\n",
    "    return w * lr_start + (1 - w) * lr_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu116\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy --pre torch torchvision torchaudio --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bv/ll6j7g452_15zbvc974c7y7w0000gn/T/ipykernel_42814/1772103530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskedAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Initial weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# Build model\n",
    "device = 'cuda'\n",
    "\n",
    "# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\n",
    "model = MaskedAutoencoder(15, units=[512, 512, 512, 512, 512, 256, 128]).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=cosine_decay)\n",
    "loss_fn = MaskedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mask_n_rows() missing 1 required positional argument: 'n_missing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bv/ll6j7g452_15zbvc974c7y7w0000gn/T/ipykernel_36460/1693033719.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Validation Mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mvalidation_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmask_n_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m validation_prob = list(data[f4_col].isna().sum(axis=1).value_counts() \\\n\u001b[1;32m     16\u001b[0m     / data.loc[data[f4_col].isna().sum(axis=1)>0, f4_col].isna().sum(axis=1).value_counts().sum())[1:]\n",
      "\u001b[0;32m/var/folders/bv/ll6j7g452_15zbvc974c7y7w0000gn/T/ipykernel_36460/1693033719.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Validation Mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mvalidation_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmask_n_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m validation_prob = list(data[f4_col].isna().sum(axis=1).value_counts() \\\n\u001b[1;32m     16\u001b[0m     / data.loc[data[f4_col].isna().sum(axis=1)>0, f4_col].isna().sum(axis=1).value_counts().sum())[1:]\n",
      "\u001b[0;31mTypeError\u001b[0m: mask_n_rows() missing 1 required positional argument: 'n_missing'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# for epoch in epochs,\n",
    "np.random.seed(6)\n",
    "\n",
    "n = X_mixed.shape[0]\n",
    "batch_size = 4096\n",
    "n_batches = n // batch_size + 1\n",
    "index = np.arange(n)\n",
    "\n",
    "valid_per = 5\n",
    "\n",
    "# Validation Mask\n",
    "validation_masks = [mask_n_rows(Xva.shape, i+1) for i in range(5)]\n",
    "validation_prob = list(data[f4_col].isna().sum(axis=1).value_counts() \\\n",
    "    / data.loc[data[f4_col].isna().sum(axis=1)>0, f4_col].isna().sum(axis=1).value_counts().sum())[1:]\n",
    "\n",
    "c_scores = [np.zeros(epochs) for i in range(len(validation_masks))]\n",
    "f_scores = np.zeros(epochs)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} LR {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    losses = 0\n",
    "    norm_losses = 0\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_idx = index[i*batch_size:(i+1)*batch_size]\n",
    "        # Create batch train data\n",
    "        srce_mask = torch.tensor(srce_nan_train[batch_idx].astype(np.float32)).to(device)\n",
    "        x = torch.tensor(X_mixed[batch_idx].astype(np.float32)).to(device)\n",
    "        mask_init = torch.tensor(random_mask(x.shape, binomial_p=0.05).astype(np.float32)).to(device)\n",
    "        mask = mask_init - srce_mask * mask_init\n",
    "        x_masked = x * mask\n",
    "\n",
    "        # Forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        p = model(x_masked, mask)\n",
    "        loss = loss_fn(p, x, srce_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss # Check\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "    # Validation stepb\n",
    "    if (epoch + 1) % valid_per == 0:\n",
    "        scores = []\n",
    "        for i in range(len(validation_masks)):\n",
    "            v = validate(model, validation_masks[i])\n",
    "            scores.append(v)\n",
    "            c_scores[i][epoch] = v\n",
    "            \n",
    "        final_score = math.sqrt(sum([scores[i]**2 * validation_prob[i] for i in range(len(scores))]))\n",
    "        f_scores[epoch] = final_score\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            print(f'RMSE ({i+1} rows) {scores[i]}')\n",
    "        print(f'RMSE (TDGP) {final_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1945a49d8267038c199a72e8f61b6bfcbb9f4626ddb3ddff4ec56a3f45e14a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
