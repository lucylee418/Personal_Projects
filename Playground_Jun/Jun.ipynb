{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Submissions are scored on the root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://www.kaggle.com/code/sebastianvangerwen/1st-place-solution-tps-jun-denoising-ae issued by **@SEBASTIAN VAN GERWEN**<br>\n",
    "https://towardsdatascience.com/denoising-autoencoders-dae-how-to-use-neural-networks-to-clean-up-your-data-cd9c19bc6915 issued by **@Saul Dobilas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blue Print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 81)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 81 columns):\n",
      " #   Column  Non-Null Count    Dtype  \n",
      "---  ------  --------------    -----  \n",
      " 0   row_id  1000000 non-null  int64  \n",
      " 1   F_1_0   981603 non-null   float64\n",
      " 2   F_1_1   981784 non-null   float64\n",
      " 3   F_1_2   981992 non-null   float64\n",
      " 4   F_1_3   981750 non-null   float64\n",
      " 5   F_1_4   981678 non-null   float64\n",
      " 6   F_1_5   981911 non-null   float64\n",
      " 7   F_1_6   981867 non-null   float64\n",
      " 8   F_1_7   981872 non-null   float64\n",
      " 9   F_1_8   981838 non-null   float64\n",
      " 10  F_1_9   981751 non-null   float64\n",
      " 11  F_1_10  982039 non-null   float64\n",
      " 12  F_1_11  981830 non-null   float64\n",
      " 13  F_1_12  981797 non-null   float64\n",
      " 14  F_1_13  981602 non-null   float64\n",
      " 15  F_1_14  981961 non-null   float64\n",
      " 16  F_2_0   1000000 non-null  int64  \n",
      " 17  F_2_1   1000000 non-null  int64  \n",
      " 18  F_2_2   1000000 non-null  int64  \n",
      " 19  F_2_3   1000000 non-null  int64  \n",
      " 20  F_2_4   1000000 non-null  int64  \n",
      " 21  F_2_5   1000000 non-null  int64  \n",
      " 22  F_2_6   1000000 non-null  int64  \n",
      " 23  F_2_7   1000000 non-null  int64  \n",
      " 24  F_2_8   1000000 non-null  int64  \n",
      " 25  F_2_9   1000000 non-null  int64  \n",
      " 26  F_2_10  1000000 non-null  int64  \n",
      " 27  F_2_11  1000000 non-null  int64  \n",
      " 28  F_2_12  1000000 non-null  int64  \n",
      " 29  F_2_13  1000000 non-null  int64  \n",
      " 30  F_2_14  1000000 non-null  int64  \n",
      " 31  F_2_15  1000000 non-null  int64  \n",
      " 32  F_2_16  1000000 non-null  int64  \n",
      " 33  F_2_17  1000000 non-null  int64  \n",
      " 34  F_2_18  1000000 non-null  int64  \n",
      " 35  F_2_19  1000000 non-null  int64  \n",
      " 36  F_2_20  1000000 non-null  int64  \n",
      " 37  F_2_21  1000000 non-null  int64  \n",
      " 38  F_2_22  1000000 non-null  int64  \n",
      " 39  F_2_23  1000000 non-null  int64  \n",
      " 40  F_2_24  1000000 non-null  int64  \n",
      " 41  F_3_0   981971 non-null   float64\n",
      " 42  F_3_1   981655 non-null   float64\n",
      " 43  F_3_2   981944 non-null   float64\n",
      " 44  F_3_3   981946 non-null   float64\n",
      " 45  F_3_4   981627 non-null   float64\n",
      " 46  F_3_5   981702 non-null   float64\n",
      " 47  F_3_6   981808 non-null   float64\n",
      " 48  F_3_7   981987 non-null   float64\n",
      " 49  F_3_8   981902 non-null   float64\n",
      " 50  F_3_9   981894 non-null   float64\n",
      " 51  F_3_10  981800 non-null   float64\n",
      " 52  F_3_11  981612 non-null   float64\n",
      " 53  F_3_12  981703 non-null   float64\n",
      " 54  F_3_13  981940 non-null   float64\n",
      " 55  F_3_14  981861 non-null   float64\n",
      " 56  F_3_15  981762 non-null   float64\n",
      " 57  F_3_16  981878 non-null   float64\n",
      " 58  F_3_17  981722 non-null   float64\n",
      " 59  F_3_18  981911 non-null   float64\n",
      " 60  F_3_19  981800 non-null   float64\n",
      " 61  F_3_20  981752 non-null   float64\n",
      " 62  F_3_21  981604 non-null   float64\n",
      " 63  F_3_22  981823 non-null   float64\n",
      " 64  F_3_23  981794 non-null   float64\n",
      " 65  F_3_24  981855 non-null   float64\n",
      " 66  F_4_0   981872 non-null   float64\n",
      " 67  F_4_1   981836 non-null   float64\n",
      " 68  F_4_2   981505 non-null   float64\n",
      " 69  F_4_3   981971 non-null   float64\n",
      " 70  F_4_4   982043 non-null   float64\n",
      " 71  F_4_5   981937 non-null   float64\n",
      " 72  F_4_6   981675 non-null   float64\n",
      " 73  F_4_7   981986 non-null   float64\n",
      " 74  F_4_8   981824 non-null   float64\n",
      " 75  F_4_9   981735 non-null   float64\n",
      " 76  F_4_10  981775 non-null   float64\n",
      " 77  F_4_11  981881 non-null   float64\n",
      " 78  F_4_12  981694 non-null   float64\n",
      " 79  F_4_13  982005 non-null   float64\n",
      " 80  F_4_14  981733 non-null   float64\n",
      "dtypes: float64(55), int64(26)\n",
      "memory usage: 618.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Column `F_1_0` ~ `F_1_14`, `F_3_0` ~ `F_3_24`, `F_4_0` ~ `F_4_14` have missing values. The types of missing values are all floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F_1_0',\n",
       " 'F_1_1',\n",
       " 'F_1_2',\n",
       " 'F_1_3',\n",
       " 'F_1_4',\n",
       " 'F_1_5',\n",
       " 'F_1_6',\n",
       " 'F_1_7',\n",
       " 'F_1_8',\n",
       " 'F_1_9',\n",
       " 'F_1_10',\n",
       " 'F_1_11',\n",
       " 'F_1_12',\n",
       " 'F_1_13',\n",
       " 'F_1_14',\n",
       " 'F_2_0',\n",
       " 'F_2_1',\n",
       " 'F_2_2',\n",
       " 'F_2_3',\n",
       " 'F_2_4',\n",
       " 'F_2_5',\n",
       " 'F_2_6',\n",
       " 'F_2_7',\n",
       " 'F_2_8',\n",
       " 'F_2_9',\n",
       " 'F_2_10',\n",
       " 'F_2_11',\n",
       " 'F_2_12',\n",
       " 'F_2_13',\n",
       " 'F_2_14',\n",
       " 'F_2_15',\n",
       " 'F_2_16',\n",
       " 'F_2_17',\n",
       " 'F_2_18',\n",
       " 'F_2_19',\n",
       " 'F_2_20',\n",
       " 'F_2_21',\n",
       " 'F_2_22',\n",
       " 'F_2_23',\n",
       " 'F_2_24',\n",
       " 'F_3_0',\n",
       " 'F_3_1',\n",
       " 'F_3_2',\n",
       " 'F_3_3',\n",
       " 'F_3_4',\n",
       " 'F_3_5',\n",
       " 'F_3_6',\n",
       " 'F_3_7',\n",
       " 'F_3_8',\n",
       " 'F_3_9',\n",
       " 'F_3_10',\n",
       " 'F_3_11',\n",
       " 'F_3_12',\n",
       " 'F_3_13',\n",
       " 'F_3_14',\n",
       " 'F_3_15',\n",
       " 'F_3_16',\n",
       " 'F_3_17',\n",
       " 'F_3_18',\n",
       " 'F_3_19',\n",
       " 'F_3_20',\n",
       " 'F_3_21',\n",
       " 'F_3_22',\n",
       " 'F_3_23',\n",
       " 'F_3_24',\n",
       " 'F_4_0',\n",
       " 'F_4_1',\n",
       " 'F_4_2',\n",
       " 'F_4_3',\n",
       " 'F_4_4',\n",
       " 'F_4_5',\n",
       " 'F_4_6',\n",
       " 'F_4_7',\n",
       " 'F_4_8',\n",
       " 'F_4_9',\n",
       " 'F_4_10',\n",
       " 'F_4_11',\n",
       " 'F_4_12',\n",
       " 'F_4_13',\n",
       " 'F_4_14']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of features\n",
    "features = data.columns.drop('row_id').tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_col, f2_col, f3_col, f4_col = [], [], [], []\n",
    "for f in features:\n",
    "    if f[:3] == 'F_1':\n",
    "        f1_col.append(f)\n",
    "    elif f[:3] == 'F_2':\n",
    "        f2_col.append(f)\n",
    "    elif f[:3] == 'F_3':\n",
    "        f3_col.append(f)\n",
    "    elif f[:3] == 'F_4':\n",
    "        f4_col.append(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binomial x Random one 0 per row\n",
    "def random_mask(n, k):\n",
    "    mask = np.ones((n, k))\n",
    "\n",
    "    # Set one random per row at 0\n",
    "    mask[(np.arange(n), np.random.randint(0, k, n))] = 0\n",
    "    \n",
    "    # Add binomial probability as well\n",
    "    b_mask = np.random.binomial(1, 0.5, (n, k))    # 1 trial, p=0.5\n",
    "    return mask * b_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_n_rows(n, k, n_missing):\n",
    "    # n_missing number of indices of columns with small values\n",
    "    idx = np.random.rand(n, k).argsort(1)[:, :n_missing]\n",
    "\n",
    "    col_idx = idx.flatten()\n",
    "    row_idx = np.arange(n).repeat(n_missing)\n",
    "    \n",
    "    mask = np.ones((n, k))\n",
    "    mask[(row_idx, col_idx)] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2          True\n",
       "3         False\n",
       "4          True\n",
       "          ...  \n",
       "999995    False\n",
       "999996    False\n",
       "999997     True\n",
       "999998    False\n",
       "999999    False\n",
       "Length: 1000000, dtype: bool"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_bool = data[f4_col].isna().sum(axis=1) > 0\n",
    "missing_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_4_0</th>\n",
       "      <th>F_4_1</th>\n",
       "      <th>F_4_2</th>\n",
       "      <th>F_4_3</th>\n",
       "      <th>F_4_4</th>\n",
       "      <th>F_4_5</th>\n",
       "      <th>F_4_6</th>\n",
       "      <th>F_4_7</th>\n",
       "      <th>F_4_8</th>\n",
       "      <th>F_4_9</th>\n",
       "      <th>F_4_10</th>\n",
       "      <th>F_4_11</th>\n",
       "      <th>F_4_12</th>\n",
       "      <th>F_4_13</th>\n",
       "      <th>F_4_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.547214</td>\n",
       "      <td>1.066871</td>\n",
       "      <td>-0.134313</td>\n",
       "      <td>-0.101040</td>\n",
       "      <td>-0.660871</td>\n",
       "      <td>3.744152</td>\n",
       "      <td>0.794438</td>\n",
       "      <td>0.265185</td>\n",
       "      <td>-0.561809</td>\n",
       "      <td>0.196480</td>\n",
       "      <td>0.373434</td>\n",
       "      <td>6.206995</td>\n",
       "      <td>3.809505</td>\n",
       "      <td>1.236486</td>\n",
       "      <td>1.182055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.707374</td>\n",
       "      <td>-1.188114</td>\n",
       "      <td>-0.562419</td>\n",
       "      <td>-1.462988</td>\n",
       "      <td>1.290672</td>\n",
       "      <td>-2.895826</td>\n",
       "      <td>-0.738275</td>\n",
       "      <td>2.361818</td>\n",
       "      <td>-0.060753</td>\n",
       "      <td>0.727249</td>\n",
       "      <td>-0.271882</td>\n",
       "      <td>5.232157</td>\n",
       "      <td>-4.218259</td>\n",
       "      <td>-2.724883</td>\n",
       "      <td>-0.063775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.638262</td>\n",
       "      <td>0.546676</td>\n",
       "      <td>0.865400</td>\n",
       "      <td>-0.857077</td>\n",
       "      <td>2.667105</td>\n",
       "      <td>2.004600</td>\n",
       "      <td>-4.664806</td>\n",
       "      <td>-0.847211</td>\n",
       "      <td>-0.264249</td>\n",
       "      <td>0.664334</td>\n",
       "      <td>-0.557868</td>\n",
       "      <td>8.499483</td>\n",
       "      <td>-4.738799</td>\n",
       "      <td>-3.054611</td>\n",
       "      <td>0.494152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.851356</td>\n",
       "      <td>-3.664918</td>\n",
       "      <td>-0.508008</td>\n",
       "      <td>0.887303</td>\n",
       "      <td>0.976945</td>\n",
       "      <td>-0.359761</td>\n",
       "      <td>1.740050</td>\n",
       "      <td>1.927704</td>\n",
       "      <td>-0.082221</td>\n",
       "      <td>-0.548425</td>\n",
       "      <td>-1.186292</td>\n",
       "      <td>-2.559834</td>\n",
       "      <td>1.041985</td>\n",
       "      <td>1.934286</td>\n",
       "      <td>0.478067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.320272</td>\n",
       "      <td>-3.384869</td>\n",
       "      <td>-1.237707</td>\n",
       "      <td>-0.229380</td>\n",
       "      <td>0.228161</td>\n",
       "      <td>-2.149355</td>\n",
       "      <td>4.226621</td>\n",
       "      <td>-1.136903</td>\n",
       "      <td>0.171289</td>\n",
       "      <td>0.703419</td>\n",
       "      <td>-0.779643</td>\n",
       "      <td>4.721938</td>\n",
       "      <td>1.835678</td>\n",
       "      <td>-6.408681</td>\n",
       "      <td>0.538917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999992</th>\n",
       "      <td>0.862858</td>\n",
       "      <td>0.934280</td>\n",
       "      <td>-1.435707</td>\n",
       "      <td>-0.937750</td>\n",
       "      <td>2.457627</td>\n",
       "      <td>0.313054</td>\n",
       "      <td>4.192217</td>\n",
       "      <td>-1.922323</td>\n",
       "      <td>-0.949024</td>\n",
       "      <td>-0.626634</td>\n",
       "      <td>1.089418</td>\n",
       "      <td>11.348415</td>\n",
       "      <td>-0.030478</td>\n",
       "      <td>-2.844943</td>\n",
       "      <td>0.320233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1.148096</td>\n",
       "      <td>-5.126425</td>\n",
       "      <td>0.746223</td>\n",
       "      <td>-0.195402</td>\n",
       "      <td>2.615170</td>\n",
       "      <td>1.799592</td>\n",
       "      <td>-0.301352</td>\n",
       "      <td>5.339675</td>\n",
       "      <td>-0.991529</td>\n",
       "      <td>1.279494</td>\n",
       "      <td>-0.841051</td>\n",
       "      <td>-2.276500</td>\n",
       "      <td>1.762961</td>\n",
       "      <td>5.324553</td>\n",
       "      <td>-0.228733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>-4.990146</td>\n",
       "      <td>-1.636969</td>\n",
       "      <td>0.862797</td>\n",
       "      <td>0.331960</td>\n",
       "      <td>2.386669</td>\n",
       "      <td>1.909697</td>\n",
       "      <td>-1.299360</td>\n",
       "      <td>-0.071713</td>\n",
       "      <td>-0.162173</td>\n",
       "      <td>0.072501</td>\n",
       "      <td>-0.614687</td>\n",
       "      <td>-1.265524</td>\n",
       "      <td>0.190385</td>\n",
       "      <td>-0.344112</td>\n",
       "      <td>-0.346807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>-0.863684</td>\n",
       "      <td>3.805997</td>\n",
       "      <td>-0.189223</td>\n",
       "      <td>-0.864603</td>\n",
       "      <td>-2.608098</td>\n",
       "      <td>-1.135003</td>\n",
       "      <td>-5.127360</td>\n",
       "      <td>-3.903728</td>\n",
       "      <td>-1.597023</td>\n",
       "      <td>0.893159</td>\n",
       "      <td>0.379434</td>\n",
       "      <td>0.846266</td>\n",
       "      <td>-1.085554</td>\n",
       "      <td>3.122423</td>\n",
       "      <td>0.004831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>1.741712</td>\n",
       "      <td>-1.219797</td>\n",
       "      <td>-1.039383</td>\n",
       "      <td>0.970522</td>\n",
       "      <td>-0.285723</td>\n",
       "      <td>1.079820</td>\n",
       "      <td>-1.098772</td>\n",
       "      <td>-1.428362</td>\n",
       "      <td>-1.255175</td>\n",
       "      <td>0.509799</td>\n",
       "      <td>0.711470</td>\n",
       "      <td>-2.448386</td>\n",
       "      <td>2.334131</td>\n",
       "      <td>5.425421</td>\n",
       "      <td>-0.828847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>759268 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           F_4_0     F_4_1     F_4_2     F_4_3     F_4_4     F_4_5     F_4_6  \\\n",
       "0       5.547214  1.066871 -0.134313 -0.101040 -0.660871  3.744152  0.794438   \n",
       "1      -1.707374 -1.188114 -0.562419 -1.462988  1.290672 -2.895826 -0.738275   \n",
       "3      -2.638262  0.546676  0.865400 -0.857077  2.667105  2.004600 -4.664806   \n",
       "5       0.851356 -3.664918 -0.508008  0.887303  0.976945 -0.359761  1.740050   \n",
       "6       6.320272 -3.384869 -1.237707 -0.229380  0.228161 -2.149355  4.226621   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "999992  0.862858  0.934280 -1.435707 -0.937750  2.457627  0.313054  4.192217   \n",
       "999995  1.148096 -5.126425  0.746223 -0.195402  2.615170  1.799592 -0.301352   \n",
       "999996 -4.990146 -1.636969  0.862797  0.331960  2.386669  1.909697 -1.299360   \n",
       "999998 -0.863684  3.805997 -0.189223 -0.864603 -2.608098 -1.135003 -5.127360   \n",
       "999999  1.741712 -1.219797 -1.039383  0.970522 -0.285723  1.079820 -1.098772   \n",
       "\n",
       "           F_4_7     F_4_8     F_4_9    F_4_10     F_4_11    F_4_12    F_4_13  \\\n",
       "0       0.265185 -0.561809  0.196480  0.373434   6.206995  3.809505  1.236486   \n",
       "1       2.361818 -0.060753  0.727249 -0.271882   5.232157 -4.218259 -2.724883   \n",
       "3      -0.847211 -0.264249  0.664334 -0.557868   8.499483 -4.738799 -3.054611   \n",
       "5       1.927704 -0.082221 -0.548425 -1.186292  -2.559834  1.041985  1.934286   \n",
       "6      -1.136903  0.171289  0.703419 -0.779643   4.721938  1.835678 -6.408681   \n",
       "...          ...       ...       ...       ...        ...       ...       ...   \n",
       "999992 -1.922323 -0.949024 -0.626634  1.089418  11.348415 -0.030478 -2.844943   \n",
       "999995  5.339675 -0.991529  1.279494 -0.841051  -2.276500  1.762961  5.324553   \n",
       "999996 -0.071713 -0.162173  0.072501 -0.614687  -1.265524  0.190385 -0.344112   \n",
       "999998 -3.903728 -1.597023  0.893159  0.379434   0.846266 -1.085554  3.122423   \n",
       "999999 -1.428362 -1.255175  0.509799  0.711470  -2.448386  2.334131  5.425421   \n",
       "\n",
       "          F_4_14  \n",
       "0       1.182055  \n",
       "1      -0.063775  \n",
       "3       0.494152  \n",
       "5       0.478067  \n",
       "6       0.538917  \n",
       "...          ...  \n",
       "999992  0.320233  \n",
       "999995 -0.228733  \n",
       "999996 -0.346807  \n",
       "999998  0.004831  \n",
       "999999 -0.828847  \n",
       "\n",
       "[759268 rows x 15 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[~missing_bool, f4_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine rows with missing values\n",
    "missing_bool = data[features].isna().sum(axis=1) > 0\n",
    "\n",
    "# Define subsets of the data with row-wise missing values\n",
    "complete = data.loc[~missing_bool, features].values     # without missing values\n",
    "missing = data.loc[missing_bool, features].values       # with missing values\n",
    "\n",
    "# Split data that has no missing to use for validation set\n",
    "Xtr, Xva = train_test_split(complete)\n",
    "\n",
    "# Build training set by combining cXtr and missing data\n",
    "X_mixed = np.concatenate([Xtr, missing], axis=0)\n",
    "\n",
    "# Mask to show train values that have been imputed\n",
    "srce_nan_train = np.concatenate([\n",
    "    np.zeros(Xtr.shape),\n",
    "    data.loc[missing_bool, features].isna().astype(np.uint8).values])\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data[features].values)\n",
    "\n",
    "# Apply feature scaling\n",
    "X_mixed = np.nan_to_num(scaler.transform(X_mixed), 0.0)\n",
    "Xva = scaler.transform(Xva)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "# Dense layer with layer normalization and mish activation\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size)\n",
    "        self.act = nn.Mish()\n",
    "        self.layernorm = nn.LayerNorm(output_size, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.act(x)\n",
    "        return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Msked autoencoder model\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, n_columns, emb_dim=16,\n",
    "                 units=[512, 512, 512, 512, 512, 128]):\n",
    "        super().__init__()\n",
    "        self.n_columns = n_columns\n",
    "\n",
    "        # Embedding\n",
    "        self.inp_proj = nn.Linear(1, emb_dim)\n",
    "        self.mask_proj = nn.Linear(1, emb_dim)\n",
    "        self.emb_norm = nn.LayerNorm(n_columns * emb_dim, eps=1e-6)\n",
    "        \n",
    "        # MLP with skip connection\n",
    "        self.mlp_layers = nn.ModuleList([])\n",
    "        for i in range(len(units)):\n",
    "            if i==0:\n",
    "                input_size = n_columns * emb_dim\n",
    "            elif i==1:\n",
    "                input_size = n_columns * emb_dim + units[0]\n",
    "            else:\n",
    "                input_size = units[i-1] + units[i-2]\n",
    "            output_size = units[i]\n",
    "            self.mlp_layers.append(\n",
    "                MLP(input_size=input_size, output_size=output_size)\n",
    "            )\n",
    "                \n",
    "        self.final_dense = nn.Linear(units[-1] + units[-2], self.n_columns)\n",
    "        \n",
    "    def forward(self, inputs:torch.Tensor, mask:torch.Tensor):\n",
    "        # Embeddings\n",
    "        input_embedding = self.inp_proj(torch.unsqueeze(inputs, 2))\n",
    "        mask_embedding = self.mask_proj(torch.unsqueeze(1-mask, 2))\n",
    "        embedding = input_embedding + mask_embedding\n",
    "        embedding = torch.flatten(embedding, 1)\n",
    "        x = [self.emb_norm(embedding)]\n",
    "        \n",
    "        # MLP\n",
    "        for i in range(len(self.mlp_layers)):\n",
    "            if i==0:\n",
    "                z = self.mlp_layers[i](x[0])\n",
    "                x.append(z)\n",
    "            else:\n",
    "                z = torch.cat((x[-1], x[-2]), 1)\n",
    "                z = self.mlp_layers[i](z)\n",
    "                x.append(z)\n",
    "                \n",
    "        x = torch.cat((x[-1], x[-2]), 1)\n",
    "        x = self.final_dense(x)\n",
    "        \n",
    "        # Output modification - predict only masked values, otherwise use inputs\n",
    "        outputs = torch.mul(inputs, mask) + torch.mul(1-mask, x)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper validation method\n",
    "def validate(model, valid_mask, batch_size=4096):\n",
    "    assert valid_mask.shape == Xva.shape\n",
    "    \n",
    "    n_batches_valid = Xva.shape[0] // batch_size + 1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ps = []\n",
    "        for batch in range(n_batches_valid):\n",
    "            x = torch.tensor(Xva[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            mask = torch.tensor(valid_mask[batch * batch_size: (batch+1) * batch_size].astype(np.float32)).to(device)\n",
    "            x_masked = x * mask\n",
    "\n",
    "            p = model(x_masked, mask).cpu().numpy()\n",
    "            ps.append(p)\n",
    "\n",
    "        p = np.vstack(ps)\n",
    "        mask_bool = (1 - valid_mask).astype(bool)\n",
    "        rmse = np.sqrt(mean_squared_error(\n",
    "            scaler.inverse_transform(p)[mask_bool],\n",
    "            scaler.inverse_transform(Xva)[mask_bool]\n",
    "        ))\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to mask NaNs in the original data\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    # Mask should be 1 for masked value, 0 for unmasked value \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, inputs, target, mask):\n",
    "        loss = self.loss(inputs, target)\n",
    "        return torch.mean(loss * (1 - mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model parameters and learning rate schedule\n",
    "epochs = 300\n",
    "lr_start = 0.001\n",
    "lr_end = 0.00005\n",
    "batch_size = 4096\n",
    "\n",
    "# This cosine decay function is borrowed from AmbrosM in last month's TPS\n",
    "def cosine_decay(epoch):\n",
    "    if epochs > 1:\n",
    "        w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n",
    "    else:\n",
    "        w = 1\n",
    "    return w * lr_start + (1 - w) * lr_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bv/ll6j7g452_15zbvc974c7y7w0000gn/T/ipykernel_33546/1772103530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskedAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    984\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Initial weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# Build model\n",
    "device = 'cuda'\n",
    "\n",
    "# Final model uses units = [2048, 2048, 2048, 1024, 512, 256, 128], but I use a smaller model for this notebook\n",
    "model = MaskedAutoencoder(15, units=[512, 512, 512, 512, 512, 256, 128]).to(device)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=cosine_decay)\n",
    "loss_fn = MaskedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# for epoch in epochs...\n",
    "\n",
    "np.random.seed(6)\n",
    "\n",
    "n = X_train.shape[0]\n",
    "batch_size = 4096\n",
    "n_batches = n // batch_size + 1\n",
    "index = np.arange(n)\n",
    "\n",
    "valid_per = 5\n",
    "\n",
    "# Validation Mask\n",
    "validation_masks = [mask_n_rows(X_valid.shape, i+1) for i in range(5)]\n",
    "validation_prob = list(data[f4_col].isna().sum(axis=1).value_counts() \\\n",
    "    / data.loc[data[f4_col].isna().sum(axis=1)>0, f4_col].isna().sum(axis=1).value_counts().sum())[1:]\n",
    "\n",
    "c_scores = [np.zeros(EPOCHS) for i in range(len(validation_masks))]\n",
    "f_scores = np.zeros(EPOCHS)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} LR {optimizer.param_groups[0]['lr']}\")\n",
    "    \n",
    "    np.random.shuffle(index)\n",
    "    losses = 0\n",
    "    norm_losses = 0\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch_idx = index[i*batch_size:(i+1)*batch_size]\n",
    "        # Create batch train data\n",
    "        srce_mask = torch.tensor(srce_nan_train[batch_idx].astype(np.float32)).to(device)\n",
    "        x = torch.tensor(X_train[batch_idx].astype(np.float32)).to(device)\n",
    "        mask_init = torch.tensor(random_mask(x.shape, binomial_p=0.05).astype(np.float32)).to(device)\n",
    "        mask = mask_init - srce_mask * mask_init\n",
    "        x_masked = x * mask\n",
    "\n",
    "        # Forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        p = model(x_masked, mask)\n",
    "        loss = loss_fn(p, x, srce_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += loss # Check\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "    # Validation stepb\n",
    "    if (epoch + 1) % valid_per == 0:\n",
    "        scores = []\n",
    "        for i in range(len(validation_masks)):\n",
    "            v = validate(model, validation_masks[i])\n",
    "            scores.append(v)\n",
    "            c_scores[i][epoch] = v\n",
    "            \n",
    "        final_score = math.sqrt(sum([scores[i]**2 * validation_prob[i] for i in range(len(scores))]))\n",
    "        f_scores[epoch] = final_score\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            print(f'RMSE ({i+1} rows) {scores[i]}')\n",
    "        print(f'RMSE (TDGP) {final_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1945a49d8267038c199a72e8f61b6bfcbb9f4626ddb3ddff4ec56a3f45e14a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
